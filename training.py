# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gkv7BHLWznX1qcIpnECTJfOeROI8u6Ct
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.autograd as autograd
from collections import OrderedDict
from torch.autograd import Variable
from torch.nn import init #used for initializations
import numpy as np

def conv3x3(in_channels,out_channels,stride=1,padding=1,bias=True,groups=1):
    return nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=padding,bias=bias,groups=groups)

def upconv2x2(in_channels,out_channels,mode='transpose'):
    if mode=='transpose':
        return nn.ConvTranspose2d(in_channels,out_channels,kernel_size=2,stride=2)
    else:
        return nn.Sequential(nn.Upsample(mode='Bilinear',scale_factor=2),conv1x1(in_channels,out_channels))
    
def conv1x1(in_channels,out_channels,groups=1):
    return nn.Conv2d(in_channels,out_channels,kernel_size=1,groups=groups,stride=1)

class Downconv(nn.Module):
    def __init__(self,in_channels,out_channels,pooling=True):
        
        super(Downconv,self).__init__()
        self.in_channels=in_channels
        self.out_channels=out_channels
        self.pooling=pooling
    
        self.conv1=conv3x3(self.in_channels,self.out_channels)
        self.conv2=conv3x3(self.out_channels,self.out_channels)
    
        if self.pooling:
            self.pool=nn.MaxPool2d(kernel_size=2,stride=2)
            
    def forward(self,x):
        x=F.relu(self.conv1(x))
        x=F.relu(self.conv2(x))
        before_pool=x
        if self.pooling:
            x=self.pool(x)
        return x,before_pool    
        




class UpConv(nn.Module):
    
    def __init__(self,in_channels,out_channels,merge_mode='concat',up_mode='transpose'):
        super(UpConv,self).__init__()
        self.in_channels=in_channels
        self.out_channels=out_channels
        self.merge_mode=merge_mode
        self.up_mode=up_mode
        
        self.upconv=upconv2x2(self.in_channels,self.out_channels,mode=self.up_mode)
        
        if self.merge_mode=='concat':
            self.conv1=conv3x3(2*self.out_channels,out_channels)
        else:
            self.conv1=conv3x3(self.out_channels,self.out_channels)
        self.conv2=conv3x3(self.out_channels,self.out_channels)    
        
    def forward(self,from_down,from_up):
        #from up: tensor from decoder path
        #from_down tensor to concatenate which is the before_pool tensor in encoder arch
        from_up=self.upconv(from_up)
        if self.merge_mode=='concat':
            x=torch.cat((from_up,from_down),1) #1 gives the dimension to concat along
        else:
            x=from_up+from_down
            
        x=F.relu(self.conv1(x))
        x=F.relu(self.conv2(x))
        return x
    



class Unet(nn.Module):
    def __init__(self,num_classes,in_channels=3,depth=5,start_filts=64,up_mode='transpose',merge_mode='concat'):
        
        super(Unet,self).__init__()
        if up_mode in ('transpose','upsample'):
            self.up_mode=up_mode
        else:
            raise ValueError("\"{}\" is not a valid mode for "
                             "upsampling. Only \"transpose\" and "
                             "\"upsample\" are allowed.".format(up_mode))
            
        
        if merge_mode in ('concat','add'):
            self.merge_mode=merge_mode
        else:
            raise ValueError("\"{}\" is not a valid mode for "
                             "merging. Only \"concat\" and "
                             "\"add\" are allowed.".format(merge_mode))
        
        if self.merge_mode=='add' and self.up_mode=='upsample':
            raise ValueError("up_mode \"upsample\" is incompatible "
                             "with merge_mode \"add\" at the moment "
                             "because it doesn't make sense to use "
                             "nearest neighbour to reduce "
                             "depth channels (by half).")
            
        self.num_classes=num_classes
        self.in_channels=in_channels
        self.depth=depth
        self.start_filts=start_filts
        
        self.down_convs=[]
        self.up_convs=[]
        
        #create the encoder pathway and add to a list
        for i in range(depth):
            ins=self.in_channels if i==0 else outs
            outs=self.start_filts*(2**i)
            pooling=True if i<(depth-1) else False

            downconv=Downconv(ins,outs,pooling=pooling) 
            self.down_convs.append(downconv) #list of modules in the down path
            
        
        #create the decoder pathway and add to a list
        for i in range(depth-1):
            ins=outs
            outs=ins//2
            up_conv=UpConv(ins,outs,up_mode=up_mode,merge_mode=merge_mode)
            self.up_convs.append(up_conv) #list of modules in the up path 
            
        self.conv_final=conv1x1(outs,num_classes)
            
        #add the list of modules to the current module
        self.down_convs=nn.ModuleList(self.down_convs)
        self.up_convs=nn.ModuleList(self.up_convs)
            
        self.reset_params()
            
    #def weight_init(m):
        #if isinstance(m,nn.Conv2d):
            #init.xavier_normal(m.weight.double())
            #init.constant(m.bias,0)
            
    def reset_params(self):
        for i,m in enumerate(self.modules()):#self.modules returns an overall iterator over the modules of the net
            if isinstance(m,nn.Conv2d):
                init.xavier_normal(m.weight).double()
                init.constant(m.bias,0)
            
    def forward(self,x):
        encoder_outs=[]
        
        for i,module in enumerate(self.down_convs):
            x,before_pool=module(x)
            encoder_outs.append(before_pool)
            
        for i,module in enumerate(self.up_convs): 
            before_pool=encoder_outs[-(i+2)]
            x=module(before_pool,x)
        
        #no softmax but 1x1 conv is used for generating labels
        x=self.conv_final(x)
        return x

if __name__=='__main__':
    
    #testing on a random input
    model=Unet(3,depth=5,merge_mode='concat')
    x=Variable(torch.FloatTensor(np.random.random((1,3,320,320))))
    out = model(x)
    loss=torch.sum(out)
    loss.backward()

    print (loss)

!pip install kaggle

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle competitions download -c carvana-image-masking-challenge

!unzip /content/train.zip

!pwd

pwd

!unzip /content/train.zip -d /content/train

!unzip /content/train_masks.csv.zip -d /content/train

!unzip /content/train_masks.zip -d /content/train

import os
data_dir = "train/train/"
mask_dir = "train/train_masks/"
all_images=os.listdir(data_dir)

# Commented out IPython magic to ensure Python compatibility.
import torch 
import torch.nn as nn
import torch.optim as optim
import torch.functional as F
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')
plt.ion()
from torch.utils.data import Dataset,DataLoader
from torchvision import datasets,transforms,utils
from sklearn.model_selection import train_test_split
import os
from skimage import io
from skimage import transform as trans

!unzip sample_submission.csv

import pandas as pd 
ss = pd.read_csv("/content/sample_submission.csv" )

!unzip train_masks.csv.zip

tm = pd.read_csv("/content/train_masks.csv" )
tm

ss.rle_mask

train_image,eval_image=train_test_split(all_images,train_size=0.8,test_size=0.2)

output_size=512

class CarvanaDataset(Dataset):
    
    def __init__(self,data_dir,mask_dir,train_image,eval_image,Train=True,transforms=None):
        
        self.data_dir=data_dir
        self.mask_dir=mask_dir
        self.transforms=transforms
        self.train_image=train_image
        self.eval_image=eval_image
        self.Train=Train
        
    def __len__(self):
        if self.Train==True:
            return len(train_image)
        else:
            return len(eval_image)
        
    def __getitem__(self,idx):
        if self.Train==True:
            im=io.imread(os.path.join(self.data_dir,self.train_image[idx]))
            im_mask=io.imread(os.path.join(self.mask_dir,self.train_image[idx].split(".")[0]
                                         + '_mask.gif'))
            sample={'image':im,'mask':im_mask}
        else:
            im=io.imread(os.path.join(self.data_dir,eval_image[idx]))
            im_mask=io.imread(os.path.join(self.mask_dir,self.eval_image[idx].split(".")[0]+'_mask.gif'))
            sample={'image':im,'mask':im_mask}
                             
        if self.transforms:
            sample=self.transforms(sample)
                
        return sample

class resize(object):
    def __init__(self,output_size):
        self.output_size=output_size
        
    def __call__(self,sample):
        im,mask=sample['image'],sample['mask']
        new_h,new_w=self.output_size,self.output_size
        
        img=trans.resize(im,(new_h,new_w))
        mask_img=trans.resize(mask,(new_h,new_w))
        return {'image':img,'mask':mask_img}

class ToTensor(object):
    def __call__(self,sample):
        image,mask=sample['image'],sample['mask']
        image=image.transpose(2,0,1)
        return {'image':torch.from_numpy(image).double(),'mask':torch.from_numpy(mask).double()}

tsfm=transforms.Compose([resize(512),ToTensor()]) #composed transform to be applied
train_set=CarvanaDataset(data_dir,mask_dir,train_image,eval_image,Train=True,transforms=tsfm)
eval_set=CarvanaDataset(data_dir,mask_dir,train_image,eval_image,Train=False,transforms=tsfm)

#displaying sample images
import cv2
sample=train_set[0]
img=sample['image'].numpy().transpose(1,2,0)
msk=sample['mask'].numpy()
#img1=sample['image']
#msk1=sample['mask']
#out = model(img1)
plt.imshow(img)
plt.imshow(msk,alpha=0.5) #image with mask
#criterion(out,msk1)

train_loader=DataLoader(train_set,batch_size=1,shuffle=True)
eval_loader=DataLoader(eval_set,batch_size=1,shuffle=True)

#!pip install UNet

#from UNet import Unet

model=Unet(3,depth=5,merge_mode='concat')
optimizer=optim.SGD(model.parameters(),momentum=0.99,lr=0.01)
criterion=nn.BCELoss()

for epoch in range(1):
    running_loss=0.0
    for i,data in enumerate(train_loader):
        image=data['image'].float()
        mask=data['mask'].float()
        optimizer.zero_grad()
        output=model(image)
        
        loss=criterion(output,mask)
        loss.backward()
        optimizer.step()
        print ('image {}'.format(i))
        running_loss+=loss
        if i%100==1:
            print ("Loss={}".format(running_loss))

